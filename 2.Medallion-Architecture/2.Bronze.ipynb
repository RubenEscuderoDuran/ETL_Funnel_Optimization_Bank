{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f43abae6-d4ab-4ea6-8a5f-4519f10e523a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, IntegerType, TimestampType\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ffd5779-9553-465d-811e-801942e03a9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Now we can work and see the data in bronze folder and apply some transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70ebeebe-7e25-4578-bb3b-4d35e426f52e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df= spark.read.format('delta').load('abfss://bronze@projectamebank.dfs.core.windows.net/usuarios_banco_con_funnel')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab16cb5f-a44d-4aba-bda1-e567b8848cd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40a2731b-2271-40f3-b9e0-f91b5391ddc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## First, we observe that the data type does not correspond to certain columns. For example, date is a string, as is process_complete. We also observe that the column names are not appropriate or may cause conflicts later on.\n",
    "- Id_cliente - id_client\n",
    "- App - app,\n",
    "- date - register_date and datatype as date\n",
    "- Proceso_completo - process_complete and datatype as Int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3adf8d7-b0a3-4b5e-b8d1-46f56a9c6771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('ID_cliente', 'client_id') \\\n",
    "       .withColumnRenamed('App', 'app') \\\n",
    "       .withColumnRenamed('date', 'register_date') \\\n",
    "       .withColumnRenamed('Proceso_completo', 'complete_process') \\\n",
    "       .withColumnRenamed('fecha_ingesta', 'ingestion_date') \\\n",
    "       .withColumn('register_date',col('register_date').cast(DateType())) \\\n",
    "       .withColumn('complete_process',col('complete_process').cast(IntegerType())) \\\n",
    "       .withColumn('register_date', to_date(col('register_date'), 'yyyy/MM/dd'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69e6042c-bb3b-4a5a-a408-46cb89fbec49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### We need to do the same for clients data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bdcf934-e024-464c-9930-9e04a20f91a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_clients = spark.read.format('delta').load('abfss://bronze@projectamebank.dfs.core.windows.net/clientes')\n",
    "df_clients.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7165aee9-20e0-452e-8983-f09a90920d1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_clients.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83522ea2-c33a-46dd-a4b5-b5e1b5a6ee89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_clients = df_clients.withColumnRenamed('ID_cliente', 'client_id') \\\n",
    "                       .withColumnRenamed('Nombre', 'name') \\\n",
    "                       .withColumnRenamed('Correo','email') \\\n",
    "                       .withColumnRenamed('Edad', 'age') \\\n",
    "                       .withColumnRenamed('Estado', 'state') \\\n",
    "                       .withColumnRenamed('fecha_ingesta', 'ingestion_date') \\\n",
    "                       .withColumn('ingestion_date', to_timestamp(col('ingestion_date'), 'yyyy/MM/dd HH:mm:ss')) \\\n",
    "                       .withColumn('age', col('age').cast(IntegerType())) \\\n",
    "                       .withColumn('update_date', current_timestamp()) ##Here we add a new column update_date, This helps us when performing incremental loading, so we can see if any data has been modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "983cd3f5-f1d3-4384-990d-04d55ead683e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Now the data has been cleaned and transformed, we need to create the tables in silver container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de55c955-7b89-463d-89cb-dda36f66fa13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Once for funnel users\n",
    "\n",
    "schema_funnel = StructType([\n",
    "    StructField(\"client_id\", StringType(), True),\n",
    "    StructField(\"app\", StringType(), True),\n",
    "    StructField(\"stage\", StringType(), True),\n",
    "    StructField(\"register_date\", DateType(), True),\n",
    "    StructField(\"complete_process\", IntegerType(), True),\n",
    "    StructField(\"ingestion_date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "funnel_df = spark.createDataFrame([], schema_funnel)\n",
    "\n",
    "funnel_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"path\", \"abfss://silver@projectamebank.dfs.core.windows.net/users_funnel\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62d8bb2e-8f2d-4826-9b80-c7af6cf5dede",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Once for clients\n",
    "\n",
    "schema_clients = StructType([\n",
    "    StructField(\"client_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"ingestion_date\", TimestampType(), True),\n",
    "    StructField('update_date', TimestampType(), True)\n",
    "])\n",
    "\n",
    "clients_df = spark.createDataFrame([], schema_clients)\n",
    "\n",
    "clients_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"path\", \"abfss://silver@projectamebank.dfs.core.windows.net/catalog\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30da3e4b-6f13-470e-95df-2754e8bbffd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ok, now, we need to do a merge to insert the data in bronze container into silver table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "837c4dd7-23a6-4e03-9544-934ad45bb96e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Path for table in silver container\n",
    "silver_path = \"abfss://silver@projectamebank.dfs.core.windows.net/users_funnel\"\n",
    "\n",
    "# Load the table \n",
    "delta_silver = DeltaTable.forPath(spark, silver_path)\n",
    "\n",
    "delta_silver.alias(\"silver\").merge(\n",
    "    source=df.alias(\"bronze\"),\n",
    "    condition=\" silver.client_id = bronze.client_id and silver.stage = bronze.stage\"\n",
    ").whenNotMatchedInsert(\n",
    "    values = {\n",
    "        \"client_id\": \"bronze.client_id\",\n",
    "        \"app\": \"bronze.app\",\n",
    "        \"stage\": \"bronze.stage\",\n",
    "        \"register_date\": \"bronze.register_date\",\n",
    "        \"complete_process\": \"bronze.complete_process\",\n",
    "        \"ingestion_date\": current_timestamp()\n",
    "    }\n",
    ") .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a65ab1d-84c4-425f-8739-fe75a952d741",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Applying the same for catalog data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feb41b5a-c689-415c-a9aa-d3bc00fa9140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_path = \"abfss://silver@projectamebank.dfs.core.windows.net/catalog\"\n",
    "df_silver = spark.read.format(\"delta\").load(silver_path)\n",
    "\n",
    "delta_silver = DeltaTable.forPath(spark, silver_path)\n",
    "\n",
    "delta_silver.alias(\"silver\").merge(\n",
    "    source=df_clients.alias(\"bronze\"),\n",
    "    condition=\"silver.client_id = bronze.client_id\"\n",
    ").whenMatchedUpdate(\n",
    "    condition=\"\"\"\n",
    "        silver.name <> bronze.name OR\n",
    "        silver.email <> bronze.email OR\n",
    "        silver.age <> bronze.age OR\n",
    "        silver.state <> bronze.state\n",
    "    \"\"\",\n",
    "    set={\n",
    "        \"name\": \"bronze.name\",\n",
    "        \"email\": \"bronze.email\",\n",
    "        \"age\": \"bronze.age\",\n",
    "        \"state\": \"bronze.state\",\n",
    "        \"ingestion_date\": \"bronze.ingestion_date\",\n",
    "        \"update_date\": current_timestamp() #Here only insert the current_timestamp when the data is modified \n",
    "    }\n",
    ").whenNotMatchedInsert(values={\n",
    "    \"client_id\": \"bronze.client_id\",\n",
    "    \"name\": \"bronze.name\",\n",
    "    \"email\": \"bronze.email\",\n",
    "    \"age\": \"bronze.age\",\n",
    "    \"state\": \"bronze.state\",\n",
    "    \"ingestion_date\": \"bronze.ingestion_date\",\n",
    "    \"update_date\": 'Null' #When the data is inserted, the update_date is null because it is a new record.\n",
    "}).execute()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2.Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
